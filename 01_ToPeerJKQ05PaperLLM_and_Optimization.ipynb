{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries (including new openai)\n",
        "!pip install -q openai pandas spacy sentence-transformers openpyxl\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "FgN9ZLgXTdM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI GPT-4o client\n",
        "client = OpenAI(api_key=\"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\")  # Replace with your actual API key\n",
        "\n",
        "# Load models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Define paths\n",
        "zip_path = \"/content/KQ_FEED_Final_Meaningful_Lectures_Dataset.zip\"\n",
        "extract_dir = \"/content/dataset\"\n",
        "LECTURE_PATH = \"/content/dataset/final_meaningful_dataset/lectures\"\n",
        "OBJECTIVE_PATH = \"/content/dataset/final_meaningful_dataset/objectives\"\n"
      ],
      "metadata": {
        "id": "T8rhxWAOUGxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Show structure\n",
        "print(\"Extracted files and folders:\")\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    print(root)\n"
      ],
      "metadata": {
        "id": "Rlsp9M90UbUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(text):\n",
        "    prompt = f\"Summarize the following university-level lecture into around 1000 words:\\n\\n{text[:40000]}\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes educational lectures.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=600\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return \"ERROR\"\n"
      ],
      "metadata": {
        "id": "WZ1OT5KoUh53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_similarity(ref, gen):\n",
        "    emb1 = bert_model.encode(ref, convert_to_tensor=True)\n",
        "    emb2 = bert_model.encode(gen, convert_to_tensor=True)\n",
        "    return util.pytorch_cos_sim(emb1, emb2).item()\n",
        "\n",
        "def extract_concepts(text):\n",
        "    doc = nlp(text)\n",
        "    return set([chunk.text.lower() for chunk in doc.noun_chunks])\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "    return len(set1 & set2) / len(set1 | set2)\n",
        "\n",
        "def entropy(tokens):\n",
        "    total = len(tokens)\n",
        "    freq = Counter(tokens)\n",
        "    probs = [count / total for count in freq.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs)\n",
        "##first try of linear combination\n",
        "def compute_kqs(sem_score, kg_score, mi_score, w1=0.3, w2=0.3, w3=0.4):\n",
        "    raw_score = w1 * sem_score + w2 * kg_score + w3 * mi_score\n",
        "    return max(0.0, min(raw_score / 3.0, 1.0))  # Normalize to 0–1\n"
      ],
      "metadata": {
        "id": "8M-0icpxUm_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "lecture_files = sorted(os.listdir(LECTURE_PATH))\n",
        "objective_files = sorted(os.listdir(OBJECTIVE_PATH))\n",
        "\n",
        "for lec_file, obj_file in zip(lecture_files, objective_files):\n",
        "    lec_path = os.path.join(LECTURE_PATH, lec_file)\n",
        "    obj_path = os.path.join(OBJECTIVE_PATH, obj_file)\n",
        "\n",
        "    with open(lec_path, 'r', encoding='utf-8') as f:\n",
        "        lecture_text = f.read()\n",
        "    with open(obj_path, 'r', encoding='utf-8') as f:\n",
        "        objectives = f.read().splitlines()\n",
        "\n",
        "    gen_summary = get_summary(lecture_text)\n",
        "    sem_score = semantic_similarity(\" \".join(objectives), gen_summary)\n",
        "\n",
        "    kg_ref = extract_concepts(\" \".join(objectives))\n",
        "    kg_gen = extract_concepts(gen_summary)\n",
        "    kg_score = jaccard_similarity(kg_ref, kg_gen)\n",
        "\n",
        "    H_L = entropy(lecture_text.lower().split())\n",
        "    H_R = entropy(gen_summary.lower().split())\n",
        "    H_LR = entropy(lecture_text.lower().split() + gen_summary.lower().split())\n",
        "    mi_score = (H_L + H_R - H_LR) / max(H_L, H_R) if max(H_L, H_R) > 0 else 0.0\n",
        "\n",
        "    kqs = compute_kqs(sem_score, kg_score, mi_score)\n",
        "\n",
        "    results.append({\n",
        "        \"Lecture_File\": lec_file,\n",
        "        \"Semantic_Score\": round(sem_score, 4),\n",
        "        \"KG_Score\": round(kg_score, 4),\n",
        "        \"Mutual_Info\": round(mi_score, 4),\n",
        "        \"KQS_Score\": round(kqs, 4)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_excel(\"KQS_Batch_Results.xlsx\", index=False)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "up2voSr-Urb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UH6XAXDRTfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "import math\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# ----------------- API KEY -----------------\n",
        "# Replace with your own API key securely or via environment variable\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# ----------------- UNZIP DATA -----------------\n",
        "zip_path = \"/content/KQ_FEED_Final_Meaningful_Lectures_Dataset.zip\"\n",
        "extract_dir = \"/content/dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "LECTURE_PATH = \"/content/dataset/final_meaningful_dataset/lectures\"\n",
        "OBJECTIVE_PATH = \"/content/dataset/final_meaningful_dataset/objectives\"\n",
        "\n",
        "# ----------------- MODEL LOADING -----------------\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# ----------------- FUNCTION DEFINITIONS -----------------\n",
        "def get_summary(text):\n",
        "    prompt = f\"Summarize the following university-level lecture into around 150 words:\\n\\n{text[:3000]}\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes educational lectures.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return \"ERROR\"\n",
        "\n",
        "def semantic_similarity(ref, gen):\n",
        "    emb1 = bert_model.encode(ref, convert_to_tensor=True)\n",
        "    emb2 = bert_model.encode(gen, convert_to_tensor=True)\n",
        "    return util.pytorch_cos_sim(emb1, emb2).item()\n",
        "\n",
        "def extract_concepts(text):\n",
        "    doc = nlp(text)\n",
        "    return set([chunk.text.lower() for chunk in doc.noun_chunks])\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "    return len(set1 & set2) / len(set1 | set2)\n",
        "\n",
        "def entropy(tokens):\n",
        "    total = len(tokens)\n",
        "    freq = Counter(tokens)\n",
        "    probs = [count / total for count in freq.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs)\n",
        "\n",
        "def compute_kqs(sem_score, kg_score, mi_score, w1=0.33, w2=0.33, w3=0.33):\n",
        "    raw_score = w1 * sem_score + w2 * kg_score + w3 * mi_score\n",
        "    return max(0.0, min(raw_score / 3.0, 1.0))  # Normalize to 0–1\n",
        "\n",
        "# ----------------- BATCH PROCESSING -----------------\n",
        "results = []\n",
        "\n",
        "lecture_files = sorted(os.listdir(LECTURE_PATH))\n",
        "objective_files = sorted(os.listdir(OBJECTIVE_PATH))\n",
        "\n",
        "for lec_file, obj_file in zip(lecture_files, objective_files):\n",
        "    lec_path = os.path.join(LECTURE_PATH, lec_file)\n",
        "    obj_path = os.path.join(OBJECTIVE_PATH, obj_file)\n",
        "\n",
        "    with open(lec_path, 'r', encoding='utf-8') as f:\n",
        "        lecture_text = f.read()\n",
        "    with open(obj_path, 'r', encoding='utf-8') as f:\n",
        "        objectives = f.read().splitlines()\n",
        "\n",
        "    gen_summary = get_summary(lecture_text)\n",
        "    if gen_summary == \"ERROR\":\n",
        "        continue  # Skip in case of API failure\n",
        "\n",
        "    sem_score = semantic_similarity(\" \".join(objectives), gen_summary)\n",
        "    kg_ref = extract_concepts(\" \".join(objectives))\n",
        "    kg_gen = extract_concepts(gen_summary)\n",
        "    kg_score = jaccard_similarity(kg_ref, kg_gen)\n",
        "\n",
        "    H_L = entropy(lecture_text.lower().split())\n",
        "    H_R = entropy(gen_summary.lower().split())\n",
        "    H_LR = entropy(lecture_text.lower().split() + gen_summary.lower().split())\n",
        "    mi_score = (H_L + H_R - H_LR) / max(H_L, H_R) if max(H_L, H_R) > 0 else 0.0\n",
        "\n",
        "    kqs = compute_kqs(sem_score, kg_score, mi_score)\n",
        "\n",
        "    results.append({\n",
        "        \"Lecture_File\": lec_file,\n",
        "        \"Semantic_Score\": round(sem_score, 4),\n",
        "        \"KG_Score\": round(kg_score, 4),\n",
        "        \"Mutual_Info\": round(mi_score, 4),\n",
        "        \"KQS_Score\": round(kqs, 4)\n",
        "    })\n",
        "\n",
        "# ----------------- SAVE RESULTS -----------------\n",
        "df = pd.DataFrame(results)\n",
        "df.to_excel(\"KQS_Batch_Results.xlsx\", index=False)\n",
        "df.head()\n"
      ]
    }
  ]
}